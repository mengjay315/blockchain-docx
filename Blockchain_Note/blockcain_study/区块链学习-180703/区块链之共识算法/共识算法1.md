# 共识算法
## 共识层封装了网络节点的各类共识机制算法。
## 共识机制算法是区块链的核心技术，因为这决定了区块的产生，而记账决定方式将会影响整个系统的安全性和可靠性。
## 共识的本质是区块产生的规则.
### 共识的分类:
#### 1.Pow
#### 2.基于经济学原理(Pos,DPos, Casper)
#### 3.基于分布式系统(paxos,raft,pbft,fabric模式)
# Pow
### Proof of Work:工作量证明机制
在详细描述比特币的工作量证明前，我们有必要先聊一下密码学哈希函数。

 哈希函数是一个数学函数，具有以下三个基本特性：

1.输入可以是任意大小的字符串

2.产生固定大小的输出

3.对于特定的输入字符串，在合理的时间内可以算出哈希函数的输出

以上特性仅仅适用于一般的哈希函数，对于加密的哈希函数还需要四个附加特性：

1.碰撞阻力：如果无法找到两个值，x和y，x≠y，而H(x)=H(y)，则称哈希函数H具有碰撞阻力。

 这里要特别注意，无法找到碰撞不代表不存在碰撞，从前面的基本特性可知哈希函数的输入通常是任意长度的，而输出确是固定长度的。假设输出长度为256位，理论上如果给出2的256次幂+1个不同的输入，必然会产生碰撞。

 但是目前对人类来说这是一个天文数字，换一种说法，假设人类制造的所有计算机在宇宙起源便开始计算，到目前为止，找到该碰撞的概率仍然趋近于无穷小。

2.隐秘性：在仅知道y=H(x)的情况下，无法算出输入值x（x必须来自于一个分散的集合）。

 隐秘性保证哈希函数是一种单向函数，无法被反向计算。

3.谜题友好：对于H(x)=y，如果已知y，没有一个解决策略比只是随机的尝试x取值更容易找到x值，称这个哈希函数是谜题友好的。

 这个特性保证了哈希函数的计算除了不断重复尝试，也就是暴力计算，没有其他捷径，对于比特币的挖矿是很重要的特性。

4.雪崩效应：当哈希函数的输入值发生极微小的改变时，也会导致输出结果的剧变。

 雪崩效应保证了哈希函数输出结果具有较好的随机化特性，使其难以仅从输出结果推测输入值。

 比特币中广泛采用的哈希函数是SHA-256（Secure Hash Algorithm 256），输出的长度是256位，这个哈希函数是由美国国家安全局(NSA)所设计（哈希函数的原理属于密码学范畴，不在我们的讨论范围内）。在比特币发布的2009年，SHA-256是当时最先进的哈希加密算法，在这之后NSA又发布了SHA-384，SHA-512等更安全的加密算法（输出位数越多，找到碰撞就越困难）。

SHA-256在比特币中被广泛应用，总结下SHA-256函数在比特币中使用的位置：

•工作量证明

•比特币公钥转换为公钥哈希

•交易的输入输出部分（Transaction id，交易脚本等）

•比特币区块头Previous Block Hash

•Merkle树

比特币的工作量证明

 比特币的工作量证明是通过变动区块头(Block header)中的Nonce值，不断计算具有不同Nonce值的区块头哈希值，直到找到一个哈希值小于指定的难度值，通过发布这个结果来证明自己完成的工作量。

我们回顾一下比特币区块头的结构：

Size

Field	Description
4 bytes	Version	区块版本号，目前为2
32 bytes	Previous Block Hash	前置区块（父区块）的区块头Hash，Hash算法为double-SHA256
32 bytes	Merkle Root	区块中交易Merkle树根
4 bytes	Timestamp	区块创建UNIX时间戳
4 bytes	Difficulty Target	工作量证明算法难度
4 bytes	Nonce	通过变动该计数器来达成工作量证明要求的结果
从区块头的结构中可以看到一个4 bytes的Nonce值，Nonce值的变动会影响整个区块头的哈希值，挖矿节点即是通过尝试不同的Nonce值（通常从0开始每次加1），寻找一个哈希值小于Difficulty Target指定的难度值。

下面举个简单的例子
假设现在有一个字符串为“Satoshi Nakamoto is my dad.”，我们设定一个目标：通过在这句话末尾加上数字来计算一个哈希值，这个哈希值的十六进制表示以00开头。 

Satoshi Nakamoto is my dad.0 => 90c27bad9a86c1eb6357a9f651ab0e2fb308616e7ea0f9720d7a255491c181bc

Satoshi Nakamoto is my dad.1 => edd3247a8c56979d7741f8e98d18d729c4336f4a0234b3fd6fae3c3a943f0b6a

Satoshi Nakamoto is my dad.2 => 2b622e4cf522982bd2c7c087640e6024d856875bab97ff6329df42c19254f131

Satoshi Nakamoto is my dad.3 => cafff550474d1af4757001a5e907f74ab63607fa8bf324838d8dec530be86f0a

Satoshi Nakamoto is my dad.4 => d8468facea81c3c5964f0728f321bccbc1d76a00a23261ef158290f9ce694375

Satoshi Nakamoto is my dad.5 => f166cd0d391d1130fa866c5e9f935392dd6563f29e8e860f6e7245c43cba6c9f

Satoshi Nakamoto is my dad.6 => b2f255fe5d91abf496bafd12988283a745bd1131e223f37a7bcb8e2517636c15

Satoshi Nakamoto is my dad.7 => b53d901baf3b621362b6a1de6e0f435182b2fda45b305446f0c29b45f00c6d9f

Satoshi Nakamoto is my dad.8 => e8d41c9d69119890377983da4da97b307339a05ee3a601aeb82645422d4eab1e

Satoshi Nakamoto is my dad.9 => 008015b7e59b53a34b60785b4722002a1d7a4134e4970957beaac7fe8969ac42

最终经过10次计算，我们找到了数字9可以得到00开头的哈希值，完成了该工作量证明的哈希计算。

比特币挖矿的挑战与上例类似，区块头信息即为“Satoshi Nakamoto is my dad”，后面添加的数字9即是Nonce值。可以预知，如果我们选择的目标值开头0的个数越多，找到结果的难度就越高。

挖矿难度目标与难度调整

 比特币挖矿难度调整方式非常简单，难度目标调整即不断将256位的难度值减小，如277315号区块的难度值十六进制表示为：

0x0000000000000003A30C00000000000000000000000000000000000000000000

这个数字在二进制表示下前60位均是0，如果要增加难度只需要减小这个值，随着难度值的减小，起始0的个数增多，可寻找的哈希值范围减小，挖矿难度就越大。

 那么问题来了：这个难度值由谁来调整？如何调整？

 比特币要使出块速率恒定保持在平均10分钟一个，而随着挖矿算力飞速增加，挖矿难度必须根据这些变化进行调整。

 挖矿难度的调整是在每个完整节点中独立完成的，每过2016个区块（约2周）所有节点会检查并调整一次难度，各节点会将2016个区块的总出块时间与20160分钟比较，如果大于20160分钟则降低难度，如果小于则提升难度。

 难度调整的公式为：

New Target = Old Target * (Actual Time of Last 2016 Blocks / 20160 minutes)

为防止难度变化过大，每个周期的调整幅度不能超过4倍，如果算力变化超过4倍，多余的部分将在下一个周期调整。

 注：虽然目标调整每2,016个块发生，但是由于Bitcoin Core客户端的一个错误，实际是基于之前的2015个块的总时间（不是2016个），导致调整偏差向较高难度提高0.05%。

 当某一挖矿节点成功在第一时间找到使区块头哈希值小于目标值的Nonce值时，就将该Nonce写入区块头并立刻广播给相邻的节点，其他节点验证无误后就会将新区块记录到区块链上，并开始尝试完成下一个区块的工作量证明。

 如277315号区块的Nonce值为4215469401，区块头哈希值为：

0000000000000002a7bbd25a417c0374cc55261021e8a9ca74442b01284f0569

这个值小于难度目标值：

0000000000000003A30C00000000000000000000000000000000000000000000

值得进一步思考的问题

1.观察一下区块头的结构，你是否想过所有节点的区块头内容似乎都是一样的，这种情况下如果所有节点都从0开始尝试Nonce值，那不就永远都是算力高的节点先计算出有效的结果，算力低的节点永远没希望挖矿成功？

 答案显然是否定的，我们再仔细看下区块头的结构，确实大部分信息都是一样的，但是Merkle Root对于每个节点必然是不同的，从上一节的内容可以知道每个节点都会有自己的Coinbase交易，该交易中存在节点矿工的地址，此地址对于每个独立挖矿节点都是不同的，根据哈希函数的雪崩效应可知每个独立节点的Merkle Root必定有显著的区别。

2.比特币的工作量证明计算有一个重要的特点，即是难于计算，却易于验证，每次完成工作量证明需要大量的哈希计算，但是验证只需要一次哈希计算，使得各节点对于结果能很有效率的达成共识。

 工作量证明在我们的平时生活中其实经常发生，比如我们的考试成绩、毕业证、技能认证都是一种工作量证明，这其实是对于付出努力的一种证明。这些东西可能会较难获取，但是非常容易验证，大大提高了社会的信任效率。

3.在上一节了解到如果按照平均每十分钟产出一个区块计算，比特币理论上不再产生新币的时间约为2140年，当时我说过这个时间有可能会被提前，原因就在于挖矿难度的调整并不是实时的，而是每2016个区块调整一次。试想如果算力提升速度较快，会使区块生成速度加快，那2016个区块的产生时间会经常低于20160分钟，最终产出2100万比特币的时间也就会缩短。

4.了解完工作量证明后，这时候恐怕你会认为比特币挖矿节点为了竞争记账权不断完成哈希计算，这种计算对人类似乎毫无价值，是一种资源浪费。在这里我提供一些辩证的想法给大家参考：

 （1）比特币的挖矿表面上看目的是为了获得比特币奖励，但实际的功能是让比特币网络变得更安全。现实生活中我们为了安全（网络安全、财产安全、人生安全……）付出巨大的代价，为什么没有人觉得这是一种浪费？

 （2）比特币网络自主完成安全的铸币、交易等等一系列工作，在现实生活中这些工作通常是由银行完成，银行的经营也要付出巨大的成本，为什么没有人觉得银行是一种浪费？

 （3）人们努力寻找和挖掘黄金，往往只是因为它值钱，和比特币矿工的逐利思维是一样的。挖黄金还破坏自然环境，比特币挖矿却通常使用可再生能源，还使很多电力过剩的电厂扭亏为盈，产生了新的就业机会。为什么没有人觉得挖黄金更浪费资源？ 

 说到底我们很容易被眼前已经固化的事实所蒙蔽，对新事物的本质总是无法理解，并产生天然的抗拒。对人类来说信任必然是有代价的，而挖矿消耗的资源就是这种代价，资源消耗越多（矿工越多）这种信任越牢固（去中心化程度越高）。

当然我也很希望看到未来有技术可以使得挖矿所耗费的算力被利用起来，现在也已经有一些尝试，如将算力用来寻找素数，用于作人工智能计算等等。或者有更好的共识算法能减少资源的消耗，现在的POS,DPOS等算法都是这方面的尝试，但是否真的更好还有待时间的验证。

 综上所述，我认为我们真的不应该以此来批评比特币挖矿，单纯的认为比特币挖矿就是浪费资源是不正确的。
# Pos
### Proof of Stake:股权证明机制
POS：也称股权证明，类似于财产储存在银行，这种模式会根据你**持有数字货币的量和时间**，**分配给你相应的利息**。

就是一个根据你持有货币的量和时间，给你发利息的一个制度。在股权证明POS模式下，有一个名词叫**币龄**，每个币每天产生1币龄。
比如：比如你持有100个币，总共持有了30天，那么，此时你的币龄就为3000，这个时候，如果**你发现了一个POS区块，你的币龄就会被清空为0**。你每被清空365币龄，你将会从区块中获得0.05个币的利息(假定利息可理解为年利率5%)，那么在这个案例中，利息 = 3000 * 5% / 365 = 0.41个币。

**POS的存在主要是从经济学上的考虑和创新**。

#### 以太坊的Casper
![Casper](_v_images/20190213112455804_1317767139.png)

##### Casper FFG
Casper FFG也就是众所周知的Vitalik版Casper，是一个混合PoW/PoS共识机制。

它是正准备进行初步应用的版本，也是被精心设计好来缓冲权益证明的转变过程的。

设计的方式是，一个权益证明协议被叠加在正常的以太坊版工作量证明协议上。**虽然区块仍将通过工作量证明来挖出，每50个区块就将有一个权益证明检查点**，也就是网络中验证者评估确定性（Finality）的地方。

确定性（Finality），从一个非常宽松的意义上来说，意味着一旦一个特定的操作完成，它将永远被蚀刻在历史上，没有任何东西可以逆转这个操作。

**Casper FFG-确定性**

1. 完全经济确定性

2. 验证者没有动机

3. 意外事故处理方案




# DPoS
### Delegated Proof of Stake:股份授权证明机制
比特股的DPoS机制，中文名叫做**股份授权证明机制**（又称**受托人机制**）。
它的原理是让每一个持有比特股的人**进行投票**，由此产生101位代表 ,** 我们可以将其理解为101个超级节点或者矿池**，而这101个超级节点彼此的权利是完全相等的。

从某种角度来看，DPOS有点像是**议会制度或人民代表大会制度**。如果代表不能履行他们的职责（当轮到他们时，没能生成区块），他们会被除名，网络会选出新的超级节点来取代他们。

**DPOS的出现最主要还是因为矿机的产生**，大量的算力在不了解也不关心比特币的人身上，类似演唱会的黄牛，大量囤票而丝毫不关心演唱会的内容。


# PBFT
### 实用拜占庭容错算法
PBFT是一种**状态机副本复制算法**，即服务作为状态机进行建模，状态机在分布式系统的不同节点进行副本复制。每个状态机的副本都保存了服务的状态，同时也实现了服务的操作。

1.客户端向主节点发送请求调用服务操作
2.主节点通过广播将请求发送给其他副本
3.所有副本都执行请求并将结果发回客户端
4.客户端需要等待2f+1个不同副本节点发回相同的结果，作为整个操作的最终结果。

PBFT对每个副本节点提出了两个限定条件：
1.所有节点必须是确定性的。也就是说，在给定状态和参数相同的情况下，操作执行的结果必须相同；
2.所有节点必须从相同的状态开始执行。在这两个限定条件下，即使失效的副本节点存在，PBFT算法对所有非失效副本节点的请求执行总顺序达成一致，从而保证安全性。

**PBFT-客户端**
客户端c向主节点发送<REQUEST,o,t,c>请求执行状态机操作o，这里时间戳t用来保证客户端请求只会执行一次。客户端c发出请求的时间戳是全序排列的，后续发出的请求比早先发出的请求拥有更高的时间戳。

副本发给客户端的响应为<REPLY,v,t,c,i,r>，v是视图编号，t是时间戳，i是副本的编号，r是请求执行的结果。

客户端等待2f+1个从不同副本得到的同样响应，**同样响应需要保证签名正确**，并且具有同样的时间戳t和执行结果r。这样客户端才能把r作为正确的执行结果，因为失效的副本节点不超过f个，所以2f+1个副本的一致响应必定能够保证结果是正确有效的。

客户端会等待上一个请求完成才会发起下一个请求，但是只要能够保证请求顺序，**可以允许请求是异步的**。

**PBFT-主线流程**
当主节点p收到客户端的请求m，主节点将该请求向所有副本节点进行广播，由此一场轰轰烈烈的三阶段协议（three-phase protocol）拉开了序幕。

在预准备阶段，主节点分配一个序列号n给收到的请求，然后向所有备份节点群发预准备消息，预准备消息的格式为<<PRE-PREPARE,v,n,d>,m>，这里v是视图编号，m是客户端发送的请求消息，d是请求消息m的摘要。

进入准备阶段，所有副本节点发送准备消息<PREPARE,v,n,d,i>。

当(m,v,n,i)条件为真的时候，所有节点<COMMIT,v,n,D(m),i>向其他副本节点广播，于是就进入了确认阶段。

“请求排序协议”和“请求传输协议”进行解耦，有利于对消息传输的效率进行深度优化。

**PBFT-正常流程**
没有发生主节点失效的情况下算法的正常执行流程，其中副本0是主节点，副本3是失效节点，而C是客户端。
![PBFT-正常流程](_v_images/20190213121532346_294176912.png)


### RBFT(趣链科技)
### Robust Byzantine Fault Tolerant
###### 趣链科技自主研发区块链平台:HyperChain提供了PBFT的实现方案,然而原生的PBFT算法在可靠性与灵活性方面不够完善,Hyperchain平台对可靠性与灵活性进行了增强,设计实现了PBFT的改进算法,即RBFT.




### EOS的BFT-DPOS




### CAP原理



### ACID原则



### FLP不可能原理



### 系统模型



### 分布式算法标准



### SAMPLE




# Paxos概述










# fabric共识模式概述:
Fabric共识机制结构
共识：在Fabric中，共识过程意味着多个Peer节点对于某一批交易的发生顺序、合法性以及它们对账本状态的更新达成一致的观点。

**在Fabric中，共识是通过背书、排序和验证三个环节的保障。**

**背书过程**：背书节点对收到的来自客户端的请求（交易提案）按照自身的逻辑进行检查，以决策是否予以支持的过程。 对于调用某个链码的交易来讲，它需要获得一定条件的背书才被认为合法。
例如，必须是来自某些特定身份成员的一致同意；或者某个组织中超过一定数目的成员的支持等等；**这些背书策略可以由链码在实例化之前来指定**。

**排序过程**：对一段时间内的一批交易达成一个网络内全局一致的顺序。 Fabric中，采用可插拔式的架构，solo模式（测试使用）、Kafka等

**验证过程**：对排序后的一批交易进行提交到账本之前最终检查的过程。 验证过程包括**验证交易结构自身完整性**，**背书签名是否满足背书策略**，**交易的读写集是否满足多版本并发控制等**。 排序服务在Fabric中所有交易在交付给Committer进行验证接受之前，需要先经过排序服务进行全局排序。排序服务提供了原子广播排序功能。 **Fabric1.0架构中排序服务的功能的服务抽取出来，作为单独的fabric-orderer模块实现，代码在fabric/orderer目录下**。

**Fabric共识插件**
Solo:单节点的排序功能，实验版。 

Kafka:基于Kafka集群的排序实现。支持可持久化和可扩展性，可以在生产环境中用。

SBFT:支持BFT容错的排序实现，开发中。

**Kafka排序后端**
通过使用kafka提供排序服务，并且使用容错的方式保证多链数据的一致性。如图所示，排序服务是有kafka集群、ZK ensemble、orderer服务节点（在ordering service client 和 Kafka集群+ZK ensemble之间）。 其中ordering service client可以直接与多个OSN节点通信（orderer service node），注意osc不能直接和kafka cluster进行通信。 这些排序服务节点（OSNs）
①client身份验证，
②允许clients通过简单的接口进行读写链，
③对配置交易（这些交易要么重新配置链要么重新创建一个链）的交易的过滤和验证 

**kafka角色介绍 **
消息以主题为划分写入Kafka，一个Kafka集群包括多个主题，每个主题有包含多个分区。 每个分区是一个不断增长的、有序的、不可变的记录序列。 

![fabric1.0概述](_v_images/20190213123223676_1134769697.png)

![事务处理流](_v_images/20190213123326524_697795521.png)
