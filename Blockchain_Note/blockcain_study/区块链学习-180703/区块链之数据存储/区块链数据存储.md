# 1. 区块链与数据库
## 1.1. 比特币与数据库
比特币存储系统由普通文件和kv数据库(levelDB)组成.普通文件用于存储去区块链数据, kv数据库用于存储**区块链元数据**.
**比特币的docker环境搭建**
```
docker pull xuxinlai2002/btcorg
docker run -it xuxinlai2002/btcorg /bin/bash 
root@8ec4fdadf2d1:~/.bitcoin# bitcoind

https://github.com/btcgroup2/bitcoin

```

![dat文件](_v_images/20190209221308147_1972863506.png)
![dat文件2](_v_images/20190209221524971_445314190.png)
为了快速检索区块数据每个文件的大小是**128M Bytes**,每个区块的数据(**区块头和区块里的所有交易**) 都会**序列成字节码**的形式写入 dat 文件中.

区块的元数据格式如<blockHash, xxxxx+npos| 格式,其中xxxxx为dat文件序列号, npos为区块写入dat文件的起始位置.

交易的元数据格式如<txHash, xxxxx+npos+nTxOffset| 格式,其中xxxxx,npos和上面的描述一致,nTxOffset 为写入dat文件的起始位置(基于npos的位置).

## 1.2. 以太坊与数据库
以太坊的区块主要由区块头和交易组成,区块在存储的过程中分别将区块头和交易体经过**RLP编码**后存入kv数据库中.以太坊在数据存储的过程中,每个value对应的key都有相应的前缀,不同类型的value对应不同的前缀.

**区块交易体的存储过程如下:**

1.将区块中**交易数据和叔块头**信息进行RLP编码从而生产存储值value.

2.将数据类型前缀,编码后的区块高度和区块头哈希拼接生产key.

3.将存储至db数据库中.

**RLP(Recursive Length Prefix)**，中文翻译过来叫递归长度前缀编码，它是以太坊序列化所采用的编码方式。RLP主要用于以太坊中数据的网络传输和持久化存储。

**区块的信息可以通过区块哈希和区块高度进行检索,其存储过程如下:**

1. 区块头信息进行RLP编码从而生产value.
2. 将区块高度进行编码(转化成大端格式数据)生成encNum.
3. 将数据类型前缀(headPrefix) 和encNum 生成以区块高度为检索信息的 key.
4. 将存储至db数据库中,从而生成以区块高度为检索的信息.
5. 将数据类型前缀(blockHashPrefix) 和区块哈希生成以区块哈希为检索信息的key.
6. 将存储至db数据库中,从而生成以区块哈希为检索的信息.
在数据查询的时候,应用层只需要提供交易hash,区块高度和区块hash就能得到交易key,从而查询到相关的交易信息.

## 1.3. fabric与数据库
fabric的存储系统和比特币一样,也是由**普通文件和kv类型的数据库(levelDB/couchDB)**组成.
在fabric中,每个channel对应一个账本目录, 在账本目录中由 blockfile_00000, blockfilke_00001 命名格式的文件名组成.
为了快速检索区块数据每个文件的大小是**64 M Bytes**.每个区块的数据(**区块头和区块里的所有交易**) 都会序列成字节码的形式写入blockfile文件中.

**在序列化的过程中, 程序以 append方式打开blockfile文件,然后将区块大小和区块数据写入blockfile文件中.过程如下:**

1. 写入区块头数据, 依次写入的数据为**区块高度**, **交易哈希和前一个区块的哈希**.
2. 写入交易数据,依次写入的数据为**区块包含交易总量**和每笔交易详细数据.
3. 写入区块的Metadata数据,依次写入的数据为**Meta数据总量**和每**个Metadata项的数据详细信息**.


kv数据库中最终存储的LevelKey值由channel_name, chaincode_name和chaincode中的key值组合而成:

LevelKey = channel_name + chaincode_name +key

LevelValue 的值由BlockNum(区块号), TxNum交易在区块中的编号组成.

区块链是文件系统,这个目前不支持更改,

历史数据和区块链的索引是LevelDB,这个也不能更改.

而对于State Database,由于和业务相关,所以提供了替换数据库, 目前支持默认的LevelDB和用户可选择的CouchDB.

这里要说两点:
(1)fabric在0.6d的时候其实用的是RockDB, 但是由于License的考虑,所以在1.0改成了LevelDB.

(2)另外CouchDB也不一定是最优的,很多人还考虑到MongoDB或者MySQL等,但是由于现在fabric那边开发资源有限,所以在1.0还不会做,以后可能会实现.
![区块链数据存储](_v_images/20190210120318075_259662871.png)

**小结:**
综上所述,三种类型的区块链都使用 kv数据库存储区块链的检索信息.

在存储,检索数据上,**比特币和Hyperledger Fabric 高度一致**

即使用普通文件存储区块数据, 使用 kv数据库存储检索信息;

以太坊的区块链数据和检索信息都存储至 kv数据库中.

## 1.4. 数据库LevelDB
**特点:**

1. key和value都是任意长度的字节数组;
2. entry(即一条K-V记录)默认是按照key的字典顺序存储的,当然开发者也可以重载这个排序函数;
3. 提供的基本操作接口: Put(), Delete(), Get(), Batch();
4. 支持批量操作以原子操作进行;
5. 可以创建数据全景的snapshot(快照), 并允许在快照中查找数据.
6. 可以通过向前(或向后)迭代器便利数据 (迭代器会隐含的创建一个snapshot);
7. 自动使用Snappy压缩数据;
8. 可移植性;

**限制:**

1. 非关系型数据库模型(NoSQL),不支持sql语句,也不支持索引;
2. 一次只允许一个进程访问一个特定的数据库;
3. 没有内置的C/S架构,但开发者可以使用LevelDB库自己封装一个server;

**DEMO:levelDB源代码分析**
![demo-level](_v_images/20190210132138914_1445693338.png)

[leveldb源码分析.xlsx](file:///media/gaojie/gmj2014/myFiles/180703区块链学习/北风区块链/my-blockchain/北风徐老师课件/新的/leveldb源码分析/leveldb源码分析.xlsx)

**网上关于leveldb源码分析的文章:**

1. [leveldb源码分析--Memtable](https://www.cnblogs.com/KevinT/p/3814012.html)
2. [LevelDb 剖析之三 (内存中的数据结构Memtable)](https://www.cnblogs.com/shihaochangeworld/articles/5468804.html)
该博客下的关于leveldb的其他讲解:
![levelDB剖析](_v_images/20190210141216592_1563773267.png)

3. [LSM Tree/MemTable/SSTable基本原理](http://www.itboth.com/d/RJRbay/cassandra)
4.

**设计思路:**

1. LevelDB的数据是**存储在磁盘上的**,采用**LSM-Tree(Log-Structured Merge Tree)**,的结构实现.
2. LSM-Tree将磁盘的随机写转化为顺序写,从而大大提高了写速度. 为了做到这一点LSM-Tree的思路是将索引树结构拆成一大一小两棵树,**较小的一个常驻内存**,**较大的一个持久化到磁盘**,他们**共同维护一个有序的key空间**. 写入操作会首先操作内存中的树,随着内存中树的不断变大,会触发与磁盘中树的**归并操作**,而归并操作本身仅有顺序写.如下图所示:
![LSM-Tree](_v_images/20190210134356826_1932331434.png)

3. 随着数据的不断写入,磁盘中的树会不断膨胀,为了避免每次参与归并操作的数据量过大,以及优化读操作的考虑, LevelDB将磁盘中的数据又**拆分成多层**, 每一层的数据达到一定容量后会触发向下一层的归并操作,每一层的数据量比其上一层成倍增长. 这也就是LevelDB的名称来源.
4. 当内存中的数据达到一定的阈值, 将这部分数据真正刷新到磁盘文件中, 因而获得了极高的写性能(顺序写60MB/S, 随机写45MB/S).

**整体结构**
![整体结构](_v_images/20190210140127045_1951510308.png)

### 1.memtable
(1) leveldb的一次写入操作并不是直接将数据刷新到磁盘文件中, 而是**首先写入到内存中作为代替**. memtable就是一个在内存中进行数据组织与维护的结构.
(2) memtable中,所有的数据按用户定义的排序,等到其存储内容的容量达到阈值时**(默认为4MB)**,  便将其转换成一个不可修改的memtable, 与此同时创建一个新的memtable, 供用户继续进行读写操作.
(3) memtable底层使用了一种**跳表数据结构**, **这种数据结构效率可以比拟二叉查找树**,绝大多数操作的时间复杂度为O()log n).

### 2.immutable memtable
(1) memtable的容量达到阈值时, 变会转换成一个不可修改的memtable, 也成为immutable memtable.
(2) 这两者的结构定义完全一样,区别只是 immutable memtable是只读的.
(3) 当一个immutable memtable 被创建时, leveldb的后台压缩进程便会将利用其中的内容,创建一个sstable, 持久化到磁盘文件中.

### 3. log
leveldb的写操作并不是直接写入磁盘的, 而是先写入到内存. 假设写入到内存的数据还未来得及持久化,leveldb进程发生了异常, 抑或是宿主机发生了**宕机**, 会造成用户的写入发生丢失. 因此,leveldb在写内存之前会首先将所有的写操作写到日志文件中, 也就是log文件. 当以下异常情况发生时,均可以通过日志文件进行恢复:
1.写log期间进程异常;
2.写log完成，写内存未完成;
3. write动作完成(即log、内存写入都完成)后，进程异常;
4. Immutable memtable持久化过程中进程异常;
5. 其他压缩异常
当第一类情况发生时，数据库重启读取log时，发现异常日志数据，**抛弃该条日志数据**，即视作这次用户写 入失败，保障了数据库的一致性;
当第二类，第三类，第四类情况发生了，均可以通过**redo**日志文件中记录的写入操作完成数据库的**恢复**。 每次日志的写操作都是一次顺序写，因此**写效率高，整体写入性能较好.
**此外，leveldb的用户写操作的**原子性**同样通过日志来实现.

### 3.1 log的结构
数据写入Memtable之前，会首先顺序写入Log文件，以避免数据丢失。LevelDB实例启动时会从Log文件中恢复Memtable内容。
. 磁盘存储
. 大量的Append操作
. 没有删除单条数据的操作
. 遍历的读操作
![log文件结构](_v_images/20190210192751492_1931194104.png)
![Block](_v_images/20190210192817435_863728268.png)
Log文件又划分为固定大小的Block单位，并保证Block的开始位置一定是一个新的Record。

## 4.sstable
sstable就是下图的.ldb文件:
![sstable:0000002.ldb文件](_v_images/20190210225254604_1820546753.png)
虽然leveldb采用了先写内存的方式来提高写入效率，但是内存中数据不可能无限增长，且日志中记录的写入 操作过多，会导致异常发生时，恢复时间过长.**因此内存中的数据达到一定容量，就需要将数据持久化到磁盘中**.
除了**某些元数据文件**，leveldb的数据主要都是**通过sstable来进行存储.**
虽然在内存中, 所有的数据都是按序排列的，但是当多个memetable数据持久化到磁盘后，对应的不同 的sstable之间是存在交集的，在读操作时，需要对所有的sstable文件进行遍历，严重影响了读取效率。
因 此leveldb后台会“定期“整合这些sstable文件，该过程也称为**compaction**. 随着compaction的进行，sstable文件 在逻辑上被分成若干层，**由内存数据直接dump出来的文件称为level 0层文件**，后期整合而成的文件为level i 层文件，这也是leveldb这个名字的由来。
注意，**所有的sstable文件本身的内容是不可修改的**，这种设计哲学为leveldb带来了许多优势，简化了很多设 计.

**sstable概述**
leveldb是典型的LSM树(Log Structured-Merge Tree)实现，即一次leveldb的写入过程并不 是直接将数据持久化到磁盘文件中，而是将写操作首先写入日志文件中，其次将写操作应用在memtable上。 当leveldb达到**checkpoint点**(**memtable中的数据量超过了预设的阈值**)，会将当前memtable冻结成一个不可 更改的内存数据库(immutable memory db)，**并且创建一个新的memtable供系统继续使用**。
leveldb(或者说LSM树)设计Minor Compaction的目的是为了:
1. **有效地降低内存的使用率**;
2. **避免日志文件过大，系统恢复时间过长**;
当memory db的数据被持久化到文件中时，leveldb将以一定规则进行文件组织，这种文件格式成为**sstable**。

**sstable文件格式-物理结构: **
为了提高整体的读写效率，一个sstable文件按照固定大小进行块划分，默认每个块的大小为4KiB。**每 个Block中，除了存储数据以外，还会存储两个额外的辅助字段**:
1. **压缩类型**
2. **CRC校验码** 压缩类型说明了Block中存储的数据是否进行了数据压缩，若是，采用了哪种算法进行压缩。leveldb中默认 采用Snappy算法进行压缩。 CRC校验码是**循环冗余校验校验码**，**校验范围包括数据以及压缩类型**。
![sstable文件格式-物理结构](_v_images/20190211154615979_2041169936.png)

**sstable文件格式-逻辑结构**

在逻辑上，根据功能不同，leveldb在逻辑上又将sstable分为:

1.data block: 用来存储key value数据对;

2.filter block: 用来存储一些过滤器相关的数据(**布隆过滤器**)，**但是若用户不指定leveldb使用过滤器，leveldb在该block中不会存储任何内容**;

3.meta Index block: 用来存储filter block的索引信息(索引信息**指在该sstable文件中的偏移量以及数据长度**);

4.index block:index block中用来存储每个data block的索引信息;

5.footer: 用来存储meta index block及index block的索引信息;

注意，1-4类型的区块，其物理结构都是和上面一样，每个区块都会有自己的压缩信息以及CRC校验码信息。

![逻辑结构](_v_images/20190211160737045_939568410.png)

**data block结构:**

data block中存储的数据是leveldb中的key/value键值对.

其中一个data block中的数据部分(不包括压缩类型、CRC校验码)按逻辑又以下图进行划分:

第一部分用来存储key/value数据。由于sstable中所有的key/value对都是严格**按序存储**的，为了节省存储空 间，leveldb并不会为每一对keyvalue对都存储完整的key值，
而是存储与上一个key非共享的部分，**避免 了key重复内容的存储**。**每间隔若干个keyvalue对，将为该条记录重新存储一个完整的key**。**重复该过程(默认间隔值为16)**，每个重新存储完整key的点称之为**Restart point**。
![data block 结构](_v_images/20190211162026933_1178189842.png)

**filter block结构  **

filter block存储的是data block数据的一些过滤信息。这些过滤数据一般指代布隆过滤器的数据，用于加快查询的速度;

filter block存储的数据主要可以分为两部分:

(1)过滤数据

(2)索引数据

Base Lg默认值为11，表示每2KB的数据，创建一个新的过滤器来存放过滤数据。一个sstable只有一个filter block，其内存储了所有block的filter数据.具体来说，filter_data_k
包含了所有起始 位置处于 [base*k, base*(k+1)]范围内的block的key的集合的filter数据，**按数据大小而非block切分主要是为了 尽量均匀**，以应对存在一些block的key很多，另一些block的key很少的情况。
![filter block](_v_images/20190211163117778_87228962.png)

**meta index block结构**

meta index block用来存储filter block在整个sstable中的索引信息。
meta index block只存储一条记录:
该记录的key为:"filter."与过滤器名字组成的常量字符串
该记录的value为:filter block在sstable中的索引信息序列化后的内容，
索引信息包括:
(1)在sstable中的偏 移量
(2)数据长度。

**index block结构 **

与meta index block类似，index block用来存储所有data block的相关索引信息。
index block包含若干条记录，每一条记录代表一个data block的索引信息。
一条索引包括以下内容:
1.data block i 中最大的key值;
2. 该data block起始地址在sstable中的偏移量;
3. 该data block的大小;
![index block结构](_v_images/20190211164524461_84050354.png)

**footer结构 **

footer大小固定，为48字节，用来存储meta index block与index block在sstable中的索引信息，另外尾部还会存储一个**magic word**，内容为:"http://code.google.com/p/leveldb/"字符串sha1哈希的前8个字节。
![footer结构](_v_images/20190211164741613_2122993079.png)

**sstable写操作 **

sstable的写操作通常发生在:
• memory db将内容持久化到磁盘文件中时，会创建一个sstable进行写入;
• leveldb后台进行文件compaction时，会将若干个sstable文件的内容重新组织，输出到若干个新 的sstable文件中;
对sstable进行写操作的数据结构为tWriter，具体定义如下:
![sstable写操作](_v_images/20190211165300645_1427845404.png)

主要包括了一个sstable的文件描述符，
底层文件系统的writer，该sstable中所有数据项最大最小的key值, 以及 一个内嵌的tableWriter。

**tableWriter**

理解tableWriter的Append函数是理解整个写入过程的关键

其 中blockWriter与filterWriter表 示 底 层 的 两 种 不 同 的writer，blockWriter负 责 写 入data数 据 的 写 入 ， 而filterWriter负责写入过滤数据。
pendingBH记录了上一个dataBlock的索引信息，当下一个dataBlock的数据开始写入时，将该索引信息写 入indexBlock中。

![tableWriter](_v_images/20190211165819494_537663856.png)


## 5.manifest
leveldb中有个版本的概念，一个版本中主要记录了每一层中所有文件的元数据，
元数据包括:
1.文件大小
2.最大key值
3.最小key值

该版本信息十分关键，除了在查找数据时，利用维护的每个文件的最大/ 小key值来加快查找，还在其中维护了一些进行compaction的统计值，来控制compaction的进行.
![manifest](_v_images/20190210223209230_1033050663.png)

以goleveldb为例，一个文件的元数据主要包括了最大最小key，文件大小等信息;

manifest文件专用于记录版本信息。leveldb采用了**增量式**的存储方式，**记录每一个版本相较于上一个版本的 变化情况**。
展开来说**，一个Manifest文件中，包含了多条Session Record**。一个Session Record记录了从上一个版本至该版 本的变化情况。

注解: 变化情况大致包括:
(1)新增了哪些sstable文件;
(2)删除了哪些sstable文件(由于compaction导致);
(3)最新的journal日志文件标号等;

**Manifest结构 **

借助这个Manifest文件，leveldb启动时，可以根据一个初始的版本状态，不断地应用这些版本改动，使得系 统的版本信息恢复到最近一次使用的状态。
一个Manifest文件的格式示意图如下所示:一个Manifest内部包含若干条Session Record**，其中第一条Session Record记载了当时leveldb的全量版本信息**，其余若干条Session Record仅记录每次更迭的变化情况。
![Manifest结构](_v_images/20190212135131816_1513184737.png)
因此，**每个manifest文件的第一条Session Record都是一个记录点，记载了全量的版本信息，可以作为一个初 始的状态进行版本恢复**。

**Manifest的Commit**

每当完成一次major compaction整理内部数据或者通过minor compaction或者重启阶段的日志重放新生成一个0层文件，都会触发leveldb进行一个版本升级。

注解: 注意，对于leveldb来说，
增减某些sstable文件需要作为一个原子性操作，
状态变更前后需要保持数据 库的一致性。

在整个过程中，原子性体现在:
整个操作的完成标志为manifest文件中完整的写入了一条session record，在 此之前，即便某些文件写入失败导致进程退出，数据库重启启动时，仍然能够恢复到崩溃之前正确的状 态，而将这些无用的sstable文件删除，重新进行compaction动作。 一致性体现在:leveldb状态变更的操作都是以version更新为标记，而version更新是整个流程的最后一步，
因此数据库必然都是从一个一致性的状态变更到另外一个一致性的状态。


## 6.主流程
1. 采用了op log，它就可以把随机的磁盘写操作，变成了对op log的append操作，因此提高了IO效率，最新的数据则存储在内存memtable中
2. 当op log文件大小超过限定值时，就定时做check point
3. Leveldb会生成新的Log文件和Memtable，后台调度会将Immutable Memtable的数据导出到磁盘，形成一个新的SSTable文件。
4. SSTable就是由内存中的数据不断导出并进行Compaction操作后形成的
5. SSTable的所有文件是一种层级结构，第一层为Level 0，第二层为Level 1，依次类推，层级逐渐增高，这也是为何称之为LevelDb的原因
![主流程](_v_images/20190210231519050_1132461185.png)

**写操作流程:**
leveldb的一次写入分为两部分:
1. 将写操作写入日志;
2. 将写操作应用到内存数据库中; 之前已经阐述过为何这样的操作可以获得极高的写入性能，以及通过先写日志的方法能够保障用户的写入 不丢失.

注解: 其实leveldb仍然存在写入丢失的隐患。
在写完日志文件以后，操作系统并不是直接将这些数据真正落到磁盘中，**而是暂时留在操作系统缓存中**，因此当用户写入操作完成，操作系统还未来得及落盘的情况 下，**发生系统宕机**，就会造成写丢失;
但是若只是进程异常退出，则不存在该问题.

**写类型:**
leveldb对外提供的写入接口有:
(1)Put(2)Delete两种。**这两种本质对应同一种操作**，**Delete操作同样会 被转换成一个value为空的Put操作**.
除此以外，leveldb还提供了一个**批量处理的工具Batch**, 用户可以依据Batch来完成批量的数据库更新操作, 且这些操作是**原子性**的.
![写类型](_v_images/20190210232320653_208053596.png)
这个图不是很好,levelDB的层应该是成倍增加的.

**batch结构:**
无论是Put/Del操作，还是批量操作，底层都会为这些操作**创建一个batch实例**作为一个数据库操作的**最小执行单元**。因此首先介绍一下batch的组织结构.
![batch结构](_v_images/20190210232631614_1992898566.png)
在batch中，每一条数据项都按照上图格式进行编码.

每条数据项编码后的第一位是这条数据项的类型(**更新还是删除**)，

之后是数据项key的长度，数据项key的内容;

**若该数据项不是删除操作，则再加上value的 长度，value的内容**.

batch中会维护一个size值，用于表示其中包含的数据量的大小.

该size值为所有数据项key与value长度的累加，以及每条数据项额外的8个字节(CRC校验位).

这8个字节用于存储一条数据项额外的一些信息.

**key值编码**
当数据项从batch中写入到数据库中时,需要将一个key值转换, 即在leveldb内部, 所有数据项的key是进过特殊编码的, 这种格式称之为 internalkey. internalkey在用户key的基础上, 尾部追加了8个字节,用于存储 该操作对应的sequence number 和该操作的类型.
![key值编码](_v_images/20190211105112053_1238886737.png)

其中,每一个操作都会被赋予一个sequence number. 该计时器是在leveldb内部维护, 每进行一次操作就做一个累加. 由于在leveldb中, 一次更新或一次删除,采用的都是append的方式, 并非直接跟新原数据.
因此,对应同样一个key, 会有多个版本的数据记录, 而最大的sequence number对应的数据记录就是最新的.
此外, leveldb的快照(snapshot) 也是基于这个sequence number实现的, 即每一个sequence number 代表着数据库的一个版本.

**合并写:**
在leveldb中, 在面对并发写入时, 做了一个处理的优化.在同一个时刻, 只允许一个写入操作将内容写入到日志文件以及内存数据库中. 为了在写入进程较多的情况下,减少日志文件的小写入, 增加整体的写入性能, **leveldb将一些"小写入" 合并成一个 "大写入"**.
**1. 获取到写锁的写操作**
第一个写入操作获取到写入锁;
在当前写操作的数据量未超过合并上限, 且有其他写操作pending的情况下,将其他写操作的内容合并到自身;
若本次写操作的数据量超过上限,或者无其他pending的写操作了,将所有内容统一写入日志文件, 并写入到内存数据库中;
**通知每一个被合并的写操作最终的写入结果, 释放或移交写锁**;
![合并写](_v_images/20190211110850738_273319939.png)

**2.其他写操作**
等待获取写锁或者被合并;
若被合并, 判断是否合并成功, 若成功, 则等待最终写入结果;
反之, 则表明获取锁的写操作已经oversize了, 此时, 该操作直接从上个占有锁的写操作中接过写锁进行写入;
若未被合并, 则继续等待写锁或者等待被合并;
![其他写操作](_v_images/20190211111700355_2072381623.png)

**原子性**
leveldb的任意一个写操作(无论包含了多少次写)，**其原子性都是由日志文件实现的**。一个写操作中所有的内容会**以一个日志中的一条记录，作为最小单位写入**。 考虑以下两种异常情况:
1. 写日志未开始，或写日志完成一半，进程异常退出;
2. 写日志完成，进程异常退出;
前者中可能存储一个写操作的部分写已经被记载到日志文件中，仍然有部分写未被记录，这种情况下，当数据库重新启动恢复时，读到这条日志记录时，发现数据异常，**直接丢弃或退出**，实现了写入的原子性保 障.
后者，写日志已经完成，已经数据未真正持久化，数据库启动恢复时**通过redo日志实现数据写入**，**仍然保障 了原子性**。

**读操作:**
leveldb提供给用户两种进行读取数据的接口:
1. 直接通过Get接口读取数据;
2. 首先创建一个snapshot, 基于该snapshot调用Get接口读取数据.
两者的本质是一样的,只不过第一种调用方式默认地以当前数据库的状态创建了一个snapshot, 并基于此snapshot进行读取.
简单来说, 快照就是数据库在某一个时刻的状态, 基于一个快照进行数据的读取,读的内容不会因为后续数据的更改而改变.
由于两种方式本质都是基于快照进行的,接下来,先介绍以下快照,

**快照:**
快照代表着数据库某一个时刻的状态，在leveldb中，作者巧妙地用一个整型数来代表一个数据库状态。
在leveldb中，用户对同一个key的若干次修改(包括删除)是以维护多条数据项的方式进行存储的(直至进 行compaction时才会合并成同一条记录)，每条数据项都会被赋予一个序列号，代表这条数据项的新旧状 态。一条数据项的序列号越大，表示其中代表的内容为最新值。
因此，每一个序列号，其实就代表着leveldb的一个状态。换句话说，每一个序列号都可以作为一个状态快照。
当用户主动或者被动地创建一个快照时，leveldb会以当前最新的序列号对其赋值。例如图中用户在序列号 为98的时刻创建了一个快照，并且基于该快照读取key为“name”的数据时，即便此刻用户将"name"的值修改 为"dog"，再删除，用户读取到的内容仍然是“cat”。
![快照](_v_images/20190211114344001_582884417.png)

**读取:**
leveldb读取分为三步: 
1. 在memory db中查找指定的key，若搜索到符合条件的数据项，结束查找;
2. 在冻结的memory db中查找指定的key，若搜索到符合条件的数据项，结束查找;
3. 按低层至高层的顺序在level i层的sstable文件中查找指定的key，若搜索到符合条件的数据项，结束查找，否则返回Not Found错误，表示数据库中不存在指定的数据;

注解: 注意leveldb在每一层sstable中查找数据时, 都是按序依次查找sstable的。 0层的文件比较特殊。
由于0层的文件中可能存在key重合的情况，**因此在0层中，文件编号大的sstable优先查找**。理由是文件编号较大的sstable中存储的总是最新的数据。非0层文件，一层中所有文件之间的key不重合，
因此leveldb可以借助sstable的元数据(一个文件中最 小与最大的key值)进行快速定位，每一层只需要查找一个sstable文件的内容。
![读取](_v_images/20190211114658251_1779695047.png)

**日志 详解:**
为了防止写入内存的数据库因为进程异常、系统掉电等情况发生丢失，leveldb在写内存之前会将本次写操作 的内容写入日志文件中。
在leveldb中，**有两个memory db**,**以及对应的两份日志文件**。其中一个memory db是可读写的，当这个db的 数据量超过预定的上限时，**便会转换成一个不可读的memory db**，与此同时，**与之对应的日志文件也变成一 份frozen log**。
**而新生成的immutable memory db则会由后台的minor compaction进程将其转换成一个sstable文件进行持久化，持久化完成，与之对应的frozen log被删除。**
![读取操作](_v_images/20190211120022832_1178970580.png)

**日志结构**
为了增加读取效率，日志文件中按照block进行划分，每个block的大小为32KiB。**每个block中包含了若干个完整的chunk**。 一条日志记录包含一个或多个chunk。每个chunk包含了一个7字节大小的header，**前4字节是该chunk的校验码**，紧接的2字节是该chunk数据的长度，以及最后一个字节是该chunk的类型。**其中checksum校验的范围包 括chunk的类型以及随后的data数据**。
chunk共有四种类型:full，first，middle，last**。一条日志记录若只包含一个chunk**，则该chunk的类型为full。 若一条日志记录包含多个chunk，则这些chunk的第一个类型为first, 最后一个类型为last，中间包含大于等 于0个middle类型的chunk。 **由于一个block的大小为32KiB**，**因此当一条日志文件过大时，会将第一部分数据写在第一个block中，且类 型为first，若剩余的数据仍然超过一个block的大小，则第二部分数据写在第二个block中，类型为middle，最后剩余的数据写在最后一个block中，类型为last**。
![日志结构](_v_images/20190211120445894_55483116.png)

**日志内容:**
日志的内容为写入的batch编码后的信息. 具体的格式为:
![日志内容](_v_images/20190211131030093_369536093.png)
一条日志记录的内容包含:
.Header
.Data
其中header中有
(1)当前db的sequence number
(2)本次日志记录中所包含的put/del操作的个数. 紧接着写入所有batch编码后的内容。

**日志写:**
日志写入流程较为简单，在leveldb内部，实现了一个journal的writer。
首先调用Next函数获取一 个singleWriter，
这个singleWriter的作用就是写入一条journal记录.
singleWriter开始写入时，标志着第一个chunk开始写入。
在写入的过程中，不断判断writer中buffer的大小,
若超过32KiB，将chunk开始到现在做为一个完整的chunk，
为其计算header之后将整个block写入文件。
与此 同时reset buffer，开始新的chunk的写入.
若一条journal记录较大，则可能会分成几个chunk存储在若干个block中.
![日志写入](_v_images/20190211131801533_1931513921.png)

**日志读取:**
同样，日志读取也较为简单。为了避免频繁的IO读取，每次从文件中读取数据时，按block(32KiB)进行块 读取. 每次读取一条日志记录，reader调用Next函数返回一个singleReader. singleReader每次调用Read函数就返回一 个chunk的数据。每次读取一个chunk，都会检查这批数据的校验码，数据类型、数据长度等信息是否正确， 若不正确，且用户要求严格的正确性，则返回错误，否则丢弃整个chunk的数据。循环调用singleReader的read函数，直至读取到一个类型为Last的chunk，表示整条日志记录都读取完毕返回。
![日志读取](_v_images/20190211132507538_139621902.png)

## 7. 内存数据库(很重要)
leveldb中内存数据库用来维护有序的key-value对，其**底层是利用跳表实现**，绝大多数操作(读/写)的时间复杂度均为O(log n)，**有着与平衡树相媲美的操作效率**，**但是从实现的角度来说简单许多**。

**跳表(SkipList)**是由William Pugh提出的。他在论文《Skip lists: a probabilistic alternative to balanced trees》 中详细地介绍了有关跳表结构、插入删除操作的细节。这种数据结构是利用概率均衡技术，加快简化插入、删除操作，且保证绝大大多操作均拥有O(log n)的良好 效率。

**平衡树(以红黑树为代表)**是一种非常复杂的数据结构，为了维持树结构的平衡，获取稳定的查询效率， 平衡树每次插入可能会涉及到较为复杂的节点旋转等操作。作者**设计跳表的目的就是借助概率平衡，来构建一个快速且简单的数据结构，取代平衡树**。

**跳表的讲解:**

[跳表](https://blog.csdn.net/fw0124/article/details/42780679)

[跳表（skiplist）的理解](https://blog.csdn.net/weixin_41462047/article/details/81253106)

[数据结构与算法(c++)——跳跃表(skip list)](https://www.cnblogs.com/learnhow/p/6749648.html)

[SkipList跳表基本原理](https://www.cnblogs.com/a8457013/p/8251967.html)

![跳表](_v_images/20190211134535545_1585320483.png)

**内存数据库特点:**
(1)常驻内存
(2)可能会有频繁的插入和查询操作
(3)需要支持阻写状态下的遍历操作 (Immutable的Dump过程)
LevelDB采用跳表 SkipList实现, 在给提供了O(logn)的时间复杂度的同时,又非常的易于实现.
![SkipList](_v_images/20190211153540181_925986013.png)
![结构](_v_images/20190211153559404_888591775.png)
 
## 8. couchDB 数据库

**在HyperLedger Fabric中启用CouchDB作为State Database**
https://www.cnblogs.com/studyzy/p/7101136.html
